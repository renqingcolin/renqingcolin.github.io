---
layout: post
title: 视频目标检测
categories: Blog
description: 视频 目标检测 
keywords: 视频目标检测
---

## 视频预处理

1. 通过跳帧减少目标检测的实际执行开销，跳帧方法包括按照时间、按照相邻帧之间的相似性 （方法：ffmpeg）    

感知哈希算法、pHash算法和SIFT算法 [链接](https://segmentfault.com/a/1190000004467183)
```
import cv2
import numpy as np 

def cal_hash_code(path):
    img = cv2.imread(path)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    cur_gray = cv2.resize(gray, dsize=(8, 8))
    img_mean = cv2.mean(cur_gray)
    return cur_gray > img_mean[0]
def cal_phash_code(self, cur_gray):  
    # 缩小至32*32  
    m_img = cv2.resize(cur_gray, dsize=(32, 32))  
    # 浮点型用于计算  
    m_img = np.float32(m_img)  
    # 离散余弦变换，得到dct系数矩阵  
    img_dct = cv2.dct(m_img)  
    img_mean = cv2.mean(img_dct[0:8, 0:8])  
    # 返回一个8*8bool矩阵  
    return img_dct[0:8, 0:8] > img_mean[0]  
  
def cal_dhash_code(self, cur_gray):  
    # dsize=(width, height)  
    m_img = cv2.resize(cur_gray, dsize=(9, 8))  
    m_img = np.int8(m_img)  
    # 得到8*8差值矩阵  
    m_img_diff = m_img[:, :-1] - m_img[:, 1:]  
    return m_img_diff > 0
def cal_hamming_distance(bframe,frame):
    diff = np.uint8(bframe - frame) 
    return cv2.countNonZero(diff)
i1 = cal_hash_code('./opencv-3.3.0/samples/data/pic1.png')
print i1
i2 = cal_hash_code('./opencv-3.3.0/samples/data/ml.png')
print i2
print cal_hamming_distance(i1,i2)


#读取视频
cap = cv2.VideoCapture('./opencv-3.3.0/samples/data/Megamind.avi')
i1 = v
while(cap.isOpened()):
    myret,frame = cap.read()
    if myret == True: 
        i2 = cal_hash_code(frame)
        if i1 is None:
            print "dedect"
        else:
            print "dedect rate",cal_hamming_distance(i1,i2)
        i1 = i2
    else:
        break
    if cv2.waitKey(5) & 0xFF == ord('q'):
        break
cap.release()
```


## 视频目标检测框架

视频目标检测算法目前主要使用了如下的框架:  
1. 将视频帧视为独立的图像，利用图像目标检测算法获取检测结果；  
2. 利用视频的时序信息和上下文信息对检测结果进行修正； 
3. 基于高质量检测窗口的跟踪轨迹对检测结果进一步进行修正

## 单帧图像目标检测  
### 训练数据的选取
同一个视频片段背景单一，相邻多帧的图像差异较小。所以要训练现有目标检测模型，VID训练集存在大量数据冗余，并且数据多样性较差，有必要对其进行扩充。对于同样的网络，使用扩充后的数据集可以提高10%左右的检测精度。
### 网络结构选取
同样的训练数据，基于ResNet101的Faster R-CNN模型的检测精度比基于VGG16的Faster R-CNN模型的检测精度高12%左右。还有使用ResNet/Inception的基础网络、GBD-Net。

## 时序信息和上下文信息
目标在某些视频帧上会存在运动模糊，分辨率较低，遮挡等问题，即便是目前最好的图像目标检算法也不能很好地检测目标。幸运的是，视频中的时序信息和上下文信息能够帮助我们处理这类问题。  
比较有代表性的方法有：  
T-CNN中的运动指导传播（Motion-guided Propagation, MGP）  
多上下文抑制（Multi-context suppression, MCS）。

## 利用跟踪信息修正
上文提到的MGP可以填补某些视频帧上漏检的目标，但对于多帧连续漏检的目标不是很有效，而目标跟踪可以很好地解决这个问题。使用跟踪算法进一步提高视频目标检测的召回率。使用跟踪算法获取目标序列基本流程如下：  
1. 使用图像目标检测算法获取较好的检测结果；  
2. 从中选取检测得分最高的目标作为跟踪的起始锚点；  
3. 基于选取的锚点向前向后在整个视频片段上进行跟踪，生成跟踪轨迹；  
4. 从剩余目标中选择得分最高的进行跟踪，需要注意的是如果此窗口在之前的跟踪轨迹中出现过，那么直接跳过，选择下一个目标进行跟踪；  
5. 算法迭代执行，可以使用得分阈值作为终止条件  
得到的跟踪轨迹既可以用来提高目标召回率，也可以作为长序列上下文信息对结果进行修正。