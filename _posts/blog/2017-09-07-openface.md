---
layout: post
title: 通过OpenFace熟悉人脸识别框架
categories: Blog
description: 通过OpenFace熟悉人脸识别框架
keywords: OpenFace
---

#通过OpenFace熟悉人脸识别框架

### Input Image -> Detect

输入：原始的可能含有人脸的图像。
输出：人脸文职的bounding box
这一步一般我们称之为‘人脸检测’（Face Detextion），在OpenFace中，使用的是dlib、OpenCV现有的检测方法。此方法与深度学习无关，使用的特征是传统计算机视觉中的方法（一般是Hog、Haar等特征）。

对人脸检测这一步刚兴趣的可以参考下列资料：
> dlib的实现：[http://blog.dlib.net/2014/02/dlib-186-released-make-your-own-object.html](http://blog.dlib.net/2014/02/dlib-186-released-make-your-own-object.html)
></br> openCV实现：[Face Detection using Haar Cascades](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html)

### Detext -> Transform -> Crop
输入：原始图像+人脸位置bounding box
输出：“校准”过的只含有人脸的图像
对于输入的原始图像+bounding box，这一步要做的事情就是要坚持人脸中的关键点，然后根据这些关键点对人脸做对齐校准。所谓关键点，通常就是眼角的位置、鼻子的位置、脸的轮廓点等等。有了这些关键点后，我们就可以把人脸“校准”，或者说是“对齐”。解释就是原来人脸可能比较歪，这里根据关键点，使用放射变换对人脸统一“摆正”，尽量去消除姿势不同带来的误差。这一步我们一般叫做Face Alignment。
在OpenFace中，这一步同样使用的是传统方法，特点是比较快，相应的论文是：[One Millisecond Face Alignment with an Ensemble of Regression Trees](https://pdfs.semanticscholar.org/d78b/6a5b0dcaa81b1faea5fb0000045a62513567.pdf)

### Crop -> Representation
输入：校准后的单张人脸图像
输出：一个向量表示。
这一步就是使用深度卷机网络，将输入的人脸图像，转换成一个向量的表示。在OpenFace中使用的向量是128*1的，也就是一个128维的向量。

在理想的状况下，我们希望“向量表示”之间的距离就可以直接反映人脸的相似度：
>对于同一个人的人脸图像，对应的向量的欧几里得距离应该比较小。
></br>对于不同人的人脸图像，对应的向量之间的欧几里得距离应该比较大。


center loss的原始论文在这里：[http://ydwen.github.io/papers/WenECCV16.pdf](http://ydwen.github.io/papers/WenECCV16.pdf)
除了center loss外。学习人脸特征表示的方法还有很多，如triplet loss（论文地址：[A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/abs/1503.03832)）。triplet loss直接这样的用三元组（A的图像1，A的图像2，B的图像）来训练网络。去掉了最后的分类层，强迫神经网络对相同的人脸图像（三元组中的同一人A）建立统一的表达。